{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1154af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca98aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 256\n",
    "MAX_ITERS = 5000\n",
    "EVAL_INTERVAL = 250\n",
    "EVAL_ITERS = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "WARMUP_ITERS = 200\n",
    "MIN_LR = 3e-5\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DTYPE = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "PTDTYPE = {'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float32': torch.float32}[DTYPE]\n",
    "CTX = nullcontext() if DEVICE == 'cpu' else torch.amp.autocast(device_type=DEVICE, dtype=PTDTYPE)\n",
    "SCALER = torch.cuda.amp.GradScaler(enabled=(DTYPE == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed082718",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DeepSeekMoEConfig:\n",
    "\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 512\n",
    "    vocab_size:int =  50257\n",
    "    block_size: int = BLOCK_SIZE\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False\n",
    "\n",
    "    moe_n_routed_experts: int = 16\n",
    "    moe_top_k: int = 2\n",
    "    moe_expert_hidden_dim: int = 512 # (n_embd * 4) / 4\n",
    "\n",
    "    ds_moe_n_shared_experts: int = 2\n",
    "    ds_moe_shared_expert_hidden_dim: int = 1024\n",
    "\n",
    "MODEL_CONFIG = DeepSeekMoEConfig()\n",
    "\n",
    "print(\"\\n--- Notebook Configuration ---\")\n",
    "print(f\"Device: {DEVICE} with dtype: {DTYPE}\")\n",
    "print(f\"Model: {MODEL_CONFIG.n_layer} layers, {MODEL_CONFIG.n_head} heads, {MODEL_CONFIG.n_embd} embedding dim.\")\n",
    "print(f\"MoE: {MODEL_CONFIG.moe_n_routed_experts} routed experts (top {MODEL_CONFIG.moe_top_k}) + {MODEL_CONFIG.ds_moe_n_shared_experts} shared experts.\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5. Dataset and Tokenizer Configuration\n",
    "DATASET_NAME = \"roneneldan/TinyStories\"\n",
    "TRAIN_BIN_FILE = \"train.bin\"\n",
    "VAL_BIN_FILE = \"val.bin\"\n",
    "TOKENIZER_MODEL = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd26055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6. Tokenization and File Writing\n",
    "if not (os.path.exists(TRAIN_BIN_FILE) and os.path.exists(VAL_BIN_FILE)):\n",
    "    print(\"\\nBinary data files not found. Starting data download and tokenization...\")\n",
    "    ds = load_dataset(DATASET_NAME)\n",
    "    enc = tiktoken.get_encoding(TOKENIZER_MODEL)\n",
    "\n",
    "    def process_and_tokenize(example):\n",
    "        ids = enc.encode_ordinary(example['text'])\n",
    "        # Add EOS token to signify document end\n",
    "        ids.append(enc.eot_token)\n",
    "        return {'ids': ids, 'len': len(ids)}\n",
    "\n",
    "    tokenized_ds = ds.map(\n",
    "        process_and_tokenize,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"Tokenizing splits\",\n",
    "        num_proc=os.cpu_count(),\n",
    "    )\n",
    "\n",
    "    for split, dset in tokenized_ds.items():\n",
    "        split_name = 'val' if split == 'validation' else split\n",
    "        filename = f\"{split_name}.bin\"\n",
    "        arr = np.memmap(filename, dtype=np.uint16, mode='w+', shape=(sum(dset['len']),))\n",
    "        total_tokens = len(arr)\n",
    "        print(f\"Writing {total_tokens:,} tokens to {filename}...\")\n",
    "        \n",
    "        idx = 0\n",
    "        for example in tqdm(dset, desc=f\"Writing {split_name} data\"):\n",
    "            arr[idx : idx + example['len']] = example['ids']\n",
    "            idx += example['len']\n",
    "        arr.flush()\n",
    "    print(\"\\nTokenization and file writing complete.\")\n",
    "else:\n",
    "    print(\"\\nFound existing train.bin and val.bin files. Skipping data preparation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7. Data Loading Function\n",
    "def get_batch(split):\n",
    "    filename = TRAIN_BIN_FILE if split == 'train' else VAL_BIN_FILE\n",
    "    data = np.memmap(filename, dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+BLOCK_SIZE]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+BLOCK_SIZE]).astype(np.int64)) for i in ix])\n",
    "    return x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "print(\"\\nStage 1 Complete: Setup and data utilities are ready.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self , config: DeepSeekMoEConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd , 3 * config.n_embd , bias=config.bias)\n",
    "\n",
    "        self.c_proj = nn.Linear(config.n_embd , config.n_embd , bias=config.bias)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional , 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "\n",
    "            self.register_buffer(\"bias\" , torch.tril(torch.ones(config.block_size , config.block_size)).view(1,1,config.block_size , config.block_size))\n",
    "\n",
    "        \n",
    "    def forward(self , x: torch.tensor) -> torch.tensor:\n",
    "        B , T , C = x.size()\n",
    "\n",
    "        q , k , v = self.c_attn(x).split(self.n_embd , dim=2)\n",
    "\n",
    "        head_size = C // self.n_head\n",
    "        q = q.view(B , T , self.n_head , head_size).transpose(1,2)\n",
    "        k = k.view(B , T , self.n_head , head_size).transpose(1,2)\n",
    "        v = v.view(B , T , self.n_head , head_size).transpose(1,2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q , k , v , attn_mask=None , dropout_p=self.dropout if self.training else 0 , is_causal=True)\n",
    "\n",
    "        else:\n",
    "\n",
    "            attn = (q @ k.transpose(-2 , -1)) / math.sqrt(head_size)\n",
    "            attn = attn.masked_fill(self.bias[: , : , :T , :T] == 0 , float('-inf'))\n",
    "            attn = F.softmax(attn , dim=-1)\n",
    "            attn = self.attn_dropout(attn)\n",
    "\n",
    "            y = attn @ v\n",
    "\n",
    "        y = y.transpose(1,2).contiguous().view(B , T , C)\n",
    "\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "    \n",
    "dummy_config = DeepSeekMoEConfig()\n",
    "dummy_input = torch.randint(0 , dummy_config.vocab_size , (BATCH_SIZE , BLOCK_SIZE) , device=DEVICE)\n",
    "embedding_layer = nn.Embedding(dummy_config.vocab_size , dummy_config.n_embd , device=DEVICE)\n",
    "dummy_x = embedding_layer(dummy_input)\n",
    "\n",
    "mha = MultiHeadAttention(dummy_config).to(DEVICE)\n",
    "output = mha(dummy_x)\n",
    "\n",
    "print(f\"Input shape:  {dummy_x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "assert dummy_x.shape == output.shape, \"Output shape does not match input shape!\"\n",
    "print(\"MultiHeadAttention module implemented and verified successfully.\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed60126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-use the same simple MLP Expert as before.\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, n_embd: int, hidden_dim: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, hidden_dim, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, n_embd, bias=False),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2B.1. Standard MoE Layer with CORRECTED Auxiliary Loss\n",
    "# -----------------------------------------------------------------------------\n",
    "class StandardMoELayer(nn.Module):\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__()\n",
    "        self.n_routed_experts = config.moe_n_routed_experts\n",
    "        self.top_k = config.moe_top_k\n",
    "        self.experts = nn.ModuleList([Expert(config.n_embd, config.moe_expert_hidden_dim, dropout=config.dropout) for _ in range(self.n_routed_experts)])\n",
    "        self.router = nn.Linear(config.n_embd, self.n_routed_experts, bias=False)\n",
    "        self.aux_loss_coef = 1e-2\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.view(-1, C)\n",
    "        num_tokens = x_flat.shape[0]\n",
    "        \n",
    "        router_logits = self.router(x_flat)\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        topk_weights, topk_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        gates = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # --- CORRECTED Auxiliary Load Balancing Loss (as per the book) ---\n",
    "        # f_i: fraction of tokens dispatched to expert i\n",
    "        # p_i: fraction of router probability allocated to expert i\n",
    "        \n",
    "        # Calculate f_i for each expert\n",
    "        expert_counts = torch.zeros(self.n_routed_experts, device=x.device)\n",
    "        expert_counts.index_add_(0, topk_indices.view(-1), torch.ones_like(topk_indices.view(-1), dtype=torch.float))\n",
    "        f_i = expert_counts / num_tokens\n",
    "        \n",
    "        # Calculate p_i for each expert\n",
    "        p_i = routing_weights.mean(dim=0)\n",
    "        \n",
    "        # The loss is the dot product of these two distributions, scaled\n",
    "        aux_loss = self.n_routed_experts * torch.sum(p_i * f_i) * self.aux_loss_coef\n",
    "\n",
    "        # Dispatch tokens\n",
    "        final_output_flat = torch.zeros_like(x_flat)\n",
    "        for i in range(self.n_routed_experts):\n",
    "            mask = (topk_indices == i)\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)\n",
    "            if row_idx.numel() == 0: continue\n",
    "            \n",
    "            expert_in = x_flat.index_select(0, row_idx)\n",
    "            expert_out = self.experts[i](expert_in)\n",
    "            gate_values = gates[row_idx, which_k].unsqueeze(1)\n",
    "            \n",
    "            final_output_flat.index_add_(0, row_idx, expert_out * gate_values)\n",
    "            \n",
    "        return final_output_flat.view(B, T, C), aux_loss\n",
    "\n",
    "# --- The Block and Full GPT Model definitions remain the same as before ---\n",
    "\n",
    "class StandardMoEBlock(nn.Module):\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.moe = StandardMoELayer(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        moe_out, aux_loss = self.moe(self.ln_2(x))\n",
    "        x = x + moe_out\n",
    "        return x, aux_loss\n",
    "\n",
    "class StandardMoEGPT(nn.Module):\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([StandardMoEBlock(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        total_aux_loss = 0.0\n",
    "        for block in self.transformer.h:\n",
    "            x, aux_loss = block(x)\n",
    "            total_aux_loss += aux_loss\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            main_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            total_loss = main_loss + total_aux_loss / self.config.n_layer\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            total_loss = None\n",
    "        \n",
    "        return logits, total_loss\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"--- Verifying Standard MoE Baseline Model Architecture (Corrected) ---\")\n",
    "# Create a dedicated config for the Standard MoE model to adjust its parameters\n",
    "std_moe_config = DeepSeekMoEConfig()\n",
    "# To make parameters roughly equal to DeepSeekMoE (101.28M vs 82.41M), we add 6 experts\n",
    "# to compensate for the 2 large shared experts in the DeepSeek model.\n",
    "# 16 (original) + 6 (compensation) = 22\n",
    "std_moe_config.moe_n_routed_experts = 22\n",
    "\n",
    "std_moe_dummy_model = StandardMoEGPT(std_moe_config).to(DEVICE)\n",
    "dummy_input = torch.randint(0, std_moe_config.vocab_size, (BATCH_SIZE, BLOCK_SIZE), device=DEVICE)\n",
    "logits, loss = std_moe_dummy_model(dummy_input, dummy_input)\n",
    "print(f\"Dummy forward pass successful. Logits shape: {logits.shape}, Loss: {loss.item():.4f}\")\n",
    "print(f\"Total model parameters: {std_moe_dummy_model.get_num_params()/1e6:.2f}M\")\n",
    "del std_moe_dummy_model, dummy_input, logits, loss, std_moe_config\n",
    "print(\"Standard MoE architecture defined and verified successfully.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27595f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STAGE 3B: TRAINING THE STANDARD MOE BASELINE MODEL (CORRECTED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Phase 2: Training the Standard MoE Model ---\")\n",
    "\n",
    "# --- Model Initialization ---\n",
    "# FIX: Use the modified config to train the larger model as verified in the previous step\n",
    "std_moe_config = DeepSeekMoEConfig()\n",
    "std_moe_config.moe_n_routed_experts = 22\n",
    "\n",
    "standard_moe_model = StandardMoEGPT(std_moe_config)\n",
    "standard_moe_model.to(DEVICE)\n",
    "print(f\"Standard MoE Model initialized on {DEVICE}.\")\n",
    "print(f\"Total model parameters: {standard_moe_model.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = torch.optim.AdamW(standard_moe_model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "\n",
    "# --- FIX: Learning Rate Scheduler function was missing ---\n",
    "def get_lr(it):\n",
    "    if it < WARMUP_ITERS: return LEARNING_RATE * it / WARMUP_ITERS\n",
    "    if it > MAX_ITERS: return MIN_LR\n",
    "    decay_ratio = (it - WARMUP_ITERS) / (MAX_ITERS - WARMUP_ITERS)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return MIN_LR + coeff * (LEARNING_RATE - MIN_LR)\n",
    "\n",
    "# --- Loss Estimation ---\n",
    "@torch.no_grad()\n",
    "def estimate_loss_std_moe():\n",
    "    out = {}\n",
    "    standard_moe_model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split)\n",
    "            with CTX:\n",
    "                logits, loss = standard_moe_model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    standard_moe_model.train()\n",
    "    return out\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"\\nStarting Standard MoE model training...\")\n",
    "# FIX: Initialize the results dictionary since the FFN cell was removed\n",
    "comparison_results = {}\n",
    "train_losses, val_losses = [], []\n",
    "start_time = time.time()\n",
    "\n",
    "for iter_num in range(MAX_ITERS + 1):\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss_std_moe()\n",
    "        val_loss = losses['val']\n",
    "        val_losses.append(val_loss)\n",
    "        train_losses.append(losses['train'])\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {val_loss:.4f}, lr {lr:.6f}\")\n",
    "\n",
    "    X, Y = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    with CTX:\n",
    "        logits, loss = standard_moe_model(X, Y)\n",
    "\n",
    "    # Backward pass and optimizer step\n",
    "    SCALER.scale(loss).backward()\n",
    "    SCALER.step(optimizer)\n",
    "    SCALER.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "end_time = time.time()\n",
    "std_moe_train_time = (end_time - start_time) / 60\n",
    "print(f\"\\nStandard MoE Training finished in {std_moe_train_time:.2f} minutes.\")\n",
    "\n",
    "# --- Store results for comparison ---\n",
    "comparison_results['standard_moe'] = {\n",
    "    'model': standard_moe_model.cpu(),\n",
    "    'val_losses': val_losses,\n",
    "    'train_time_minutes': std_moe_train_time\n",
    "}\n",
    "print(\"Phase 2 Complete: Standard MoE model trained and results stored.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911556fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2.2. Expert Modules\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\" A standard MLP expert for the routed experts. Uses GELU activation. \"\"\"\n",
    "    def __init__(self, n_embd: int, hidden_dim: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, hidden_dim, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, n_embd, bias=False),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SharedExpert(nn.Module):\n",
    "    \"\"\" Shared experts. \"\"\"\n",
    "    def __init__(self, n_embd: int, hidden_dim: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(n_embd, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(n_embd, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(self.dropout(F.silu(self.w1(x)) * self.w3(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STAGE 2 (REVISED): DeepSeekMoELayer with Key Innovation Fixes\n",
    "# =============================================================================\n",
    "\n",
    "class DeepSeekMoELayer(nn.Module):\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__()\n",
    "        self.n_routed_experts = config.moe_n_routed_experts\n",
    "        self.top_k = config.moe_top_k\n",
    "\n",
    "        # Shared Experts (Generalists)\n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            SharedExpert(config.n_embd, config.ds_moe_shared_expert_hidden_dim, dropout=config.dropout)\n",
    "            for _ in range(config.ds_moe_n_shared_experts)\n",
    "        ])\n",
    "\n",
    "        # Routed Experts (Specialists)\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "            Expert(config.n_embd, config.moe_expert_hidden_dim, dropout=config.dropout)\n",
    "            for _ in range(self.n_routed_experts)\n",
    "        ])\n",
    "\n",
    "        # Router: A simple linear layer that outputs logits\n",
    "        self.router = nn.Linear(config.n_embd, self.n_routed_experts, bias=False)\n",
    "\n",
    "        # Non-trainable buffer for the auxiliary-loss-free bias term\n",
    "        self.register_buffer(\"router_bias\", torch.zeros(self.n_routed_experts))\n",
    "        \n",
    "        # Bias update parameters (as per reference)\n",
    "        self.bias_update_gamma = 1e-3 # Corresponds to gamma_initial\n",
    "        self.clamp_abs = 2.0 # Optional clamping for stability\n",
    "        self.tol_frac = 0.0 # Tolerance fraction, 0.0 means we always update if not perfectly balanced\n",
    "\n",
    "        # For interpretability\n",
    "        self.router_indices = None\n",
    "\n",
    "    def _affinity(self, x_flat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        FIX 1: Compute s = sigmoid(linear(x)) in fp32.\n",
    "        The router logits are computed in the model's dtype (e.g., bfloat16) for performance,\n",
    "        but the sigmoid and resulting affinity scores 's' are kept in float32 for numerical stability.\n",
    "        \"\"\"\n",
    "        router_logits = self.router(x_flat)\n",
    "        return torch.sigmoid(router_logits.float()) # Return fp32 affinity scores\n",
    "\n",
    "    def forward(self, x: torch.Tensor, analysis_mode=False):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.view(-1, C) # Shape: (B*T, C)\n",
    "\n",
    "        # --- 1. Shared Expert Path (Dense) ---\n",
    "        # FIX 3: Sum shared experts with unit weight.\n",
    "        shared_out_flat = torch.zeros_like(x_flat)\n",
    "        for expert in self.shared_experts:\n",
    "            shared_out_flat += expert(x_flat)\n",
    "\n",
    "        # --- 2. Routed Expert Path (Sparse) ---\n",
    "        # 2a. Calculate affinity scores 's' and select Top-K using 's + b'\n",
    "        s = self._affinity(x_flat) # [N, E] in fp32\n",
    "        sel_scores = s + self.router_bias.to(s.device) # Add bias for selection\n",
    "        topk_indices = torch.topk(sel_scores, self.top_k, dim=-1).indices # [N, K]\n",
    "\n",
    "        # 2b. Calculate gates from the original, unbiased 's'\n",
    "        # FIX 2: Gates = normalize(s.gather(TopK)). Bias is NEVER included in gates.\n",
    "        s_sel = s.gather(dim=1, index=topk_indices) # [N, K]\n",
    "        denom = s_sel.sum(dim=1, keepdim=True)\n",
    "        # Fallback to 1/K if sum is tiny to prevent division by zero\n",
    "        gates = torch.where(\n",
    "            denom > 1e-9,\n",
    "            s_sel / (denom + 1e-9),\n",
    "            torch.full_like(s_sel, 1.0 / self.top_k)\n",
    "        ).to(x.dtype) # [N, K]\n",
    "\n",
    "        # For interpretability\n",
    "        if analysis_mode:\n",
    "            self.router_indices = topk_indices.detach().cpu()\n",
    "\n",
    "        # 2c. Dispatch tokens and combine outputs\n",
    "        # FIX 5: Correct, efficient dispatch logic.\n",
    "        routed_out_flat = torch.zeros_like(x_flat)\n",
    "        for i in range(self.n_routed_experts):\n",
    "            # Create a mask for tokens routed to expert 'i'\n",
    "            mask = (topk_indices == i)\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)\n",
    "\n",
    "            if row_idx.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # Select the input tokens for this expert\n",
    "            expert_in = x_flat.index_select(0, row_idx)\n",
    "            # Run the expert\n",
    "            expert_out = self.routed_experts[i](expert_in)\n",
    "            # Select the gating weights for these specific tokens and expert\n",
    "            gate_values = gates[row_idx, which_k].unsqueeze(1)\n",
    "            # Weight the expert output by its gate\n",
    "            weighted_expert_out = expert_out * gate_values\n",
    "            # Add the result back to the final output tensor at the correct positions\n",
    "            routed_out_flat.index_add_(0, row_idx, weighted_expert_out)\n",
    "\n",
    "        # --- 3. Final Combination ---\n",
    "        # FIX 3: Add residual 'x' in the final output.\n",
    "        final_output = x + shared_out_flat.view(B, T, C) + routed_out_flat.view(B, T, C)\n",
    "        return final_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_bias(self):\n",
    "        \"\"\"\n",
    "        FIX 4: Implements the stepwise ¬±gamma bias update.\n",
    "        This is a simplified version of the reference, capturing the core logic.\n",
    "        A full implementation would include gamma annealing.\n",
    "        \"\"\"\n",
    "        # This method should be called with the same batch of data used in the forward pass\n",
    "        # For simplicity in our training loop, we'll recalculate the counts here.\n",
    "        # In a highly optimized framework, you might pass the counts from the forward pass.\n",
    "        \n",
    "        # We need a dummy forward pass to get the counts for the *current* bias state\n",
    "        # This is a conceptual placeholder. In the actual training loop, we'll use the\n",
    "        # counts from the last forward pass before the optimizer step.\n",
    "        # For now, let's define the logic assuming we have the `counts`.\n",
    "        # The actual call will be handled in the training loop.\n",
    "        pass # The logic will be implemented directly in the training loop for clarity.\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"--- Verifying REVISED DeepSeekMoELayer Module ---\")\n",
    "dummy_config = DeepSeekMoEConfig()\n",
    "dummy_input = torch.randn(BATCH_SIZE, BLOCK_SIZE, dummy_config.n_embd, device=DEVICE)\n",
    "\n",
    "moe_layer = DeepSeekMoELayer(dummy_config).to(DEVICE)\n",
    "output = moe_layer(dummy_input)\n",
    "\n",
    "print(f\"Input shape:  {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "assert dummy_input.shape == output.shape, \"Output shape does not match input shape!\"\n",
    "print(\"\\nREVISED DeepSeekMoELayer module implemented and verified successfully.\")\n",
    "print(\"Stage 2 Complete: All core architectural layers are defined.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STAGE 3: ASSEMBLING AND TRAINING THE FULL MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3.1. The Transformer Block\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" A single Transformer block combining Multi-Head Attention and our DeepSeekMoE layer. \"\"\"\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.moe = DeepSeekMoELayer(config)\n",
    "\n",
    "    # <--- MODIFIED: This function now returns the MoE input as well\n",
    "    def forward(self, x: torch.Tensor, analysis_mode=False):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # Capture the input to the MoE layer before it's processed\n",
    "        moe_input = self.ln_2(x)\n",
    "        # The MoE layer's output is the FFN part, which we add to the residual path\n",
    "        x = x + self.moe(moe_input, analysis_mode=analysis_mode)\n",
    "        # Return both the final output and the captured input for the bias update\n",
    "        return x, moe_input\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3.2. The Full GPT Model with DeepSeek-MoE\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# NOTE: The re-definitions of SharedExpert and DeepSeekMoELayer are redundant\n",
    "# if the previous cells were run, but we keep them here for completeness as in the original notebook.\n",
    "\n",
    "class SharedExpert(nn.Module):\n",
    "    \"\"\" A SwiGLU expert, used for the shared experts. \"\"\"\n",
    "    def __init__(self, n_embd: int, hidden_dim: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(n_embd, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(n_embd, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(self.dropout(F.silu(self.w1(x)) * self.w3(x)))\n",
    "\n",
    "class DeepSeekMoELayer(nn.Module):\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__()\n",
    "        self.n_routed_experts = config.moe_n_routed_experts\n",
    "        self.top_k = config.moe_top_k\n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            SharedExpert(config.n_embd, config.ds_moe_shared_expert_hidden_dim, dropout=config.dropout)\n",
    "            for _ in range(config.ds_moe_n_shared_experts)\n",
    "        ])\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "            Expert(config.n_embd, config.moe_expert_hidden_dim, dropout=config.dropout)\n",
    "            for _ in range(self.n_routed_experts)\n",
    "        ])\n",
    "        self.router = nn.Linear(config.n_embd, self.n_routed_experts, bias=False)\n",
    "        self.register_buffer(\"router_bias\", torch.zeros(self.n_routed_experts))\n",
    "        self.bias_update_gamma = 1e-3\n",
    "        self.router_indices = None\n",
    "\n",
    "    def _affinity(self, x_flat: torch.Tensor) -> torch.Tensor:\n",
    "        router_logits = self.router(x_flat)\n",
    "        return torch.sigmoid(router_logits.float())\n",
    "\n",
    "    def forward(self, x: torch.Tensor, analysis_mode=False):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.view(-1, C)\n",
    "        \n",
    "        shared_out_flat = torch.zeros_like(x_flat)\n",
    "        for expert in self.shared_experts:\n",
    "            shared_out_flat += expert(x_flat)\n",
    "\n",
    "        s = self._affinity(x_flat)\n",
    "        sel_scores = s + self.router_bias.to(s.device)\n",
    "        topk_indices = torch.topk(sel_scores, self.top_k, dim=-1).indices\n",
    "        \n",
    "        if analysis_mode:\n",
    "            self.router_indices = topk_indices.detach().cpu()\n",
    "\n",
    "        s_sel = s.gather(dim=1, index=topk_indices)\n",
    "        denom = s_sel.sum(dim=1, keepdim=True)\n",
    "        gates = torch.where(denom > 1e-9, s_sel / (denom + 1e-9), torch.full_like(s_sel, 1.0 / self.top_k)).to(x.dtype)\n",
    "\n",
    "        routed_out_flat = torch.zeros_like(x_flat)\n",
    "        for i in range(self.n_routed_experts):\n",
    "            mask = (topk_indices == i)\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)\n",
    "            if row_idx.numel() == 0: continue\n",
    "            expert_in = x_flat.index_select(0, row_idx)\n",
    "            expert_out = self.routed_experts[i](expert_in)\n",
    "            gate_values = gates[row_idx, which_k].unsqueeze(1)\n",
    "            weighted_expert_out = expert_out * gate_values\n",
    "            routed_out_flat.index_add_(0, row_idx, weighted_expert_out)\n",
    "\n",
    "        # As per the logic in the Block, this layer only returns the FFN output\n",
    "        return shared_out_flat.view(B, T, C) + routed_out_flat.view(B, T, C)\n",
    "\n",
    "\n",
    "class DeepSeekMoEGPT(nn.Module):\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            # This will now use the modified Block that returns two values\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # <--- MODIFIED: This function now returns router_inputs for the bias update\n",
    "    def forward(self, idx, targets=None, analysis_mode=False):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        router_inputs = [] # <--- MODIFIED: Initialize list to store MoE inputs\n",
    "        for block in self.transformer.h:\n",
    "            # <--- MODIFIED: Unpack both the output and the MoE input\n",
    "            x, moe_input = block(x, analysis_mode=analysis_mode)\n",
    "            router_inputs.append(moe_input) # <--- MODIFIED: Store the input\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        \n",
    "        # <--- MODIFIED: Return the collected router inputs\n",
    "        return logits, loss, router_inputs\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3.3. The Training Loop\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# --- Model Initialization ---\n",
    "deepseek_moe_model = DeepSeekMoEGPT(MODEL_CONFIG)\n",
    "deepseek_moe_model.to(DEVICE)\n",
    "print(f\"\\nDeepSeek-MoE Model initialized on {DEVICE}.\")\n",
    "print(f\"Total model parameters: {deepseek_moe_model.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "# --- Optimizer ---\n",
    "# FIX: The optimizer needs to be created with the correctly named model variable.\n",
    "optimizer = torch.optim.AdamW(deepseek_moe_model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "\n",
    "# --- Learning Rate Scheduler ---\n",
    "def get_lr(it):\n",
    "    if it < WARMUP_ITERS:\n",
    "        return LEARNING_RATE * it / WARMUP_ITERS\n",
    "    if it > MAX_ITERS:\n",
    "        return MIN_LR\n",
    "    decay_ratio = (it - WARMUP_ITERS) / (MAX_ITERS - WARMUP_ITERS)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return MIN_LR + coeff * (LEARNING_RATE - MIN_LR)\n",
    "\n",
    "# --- Loss Estimation ---\n",
    "@torch.no_grad()\n",
    "def estimate_loss_deepseek():\n",
    "    out = {}\n",
    "    # FIX: Use the correct model variable name here.\n",
    "    deepseek_moe_model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split)\n",
    "            with CTX:\n",
    "                # FIX: And here.\n",
    "                logits, loss, _ = deepseek_moe_model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    # FIX: And here.\n",
    "    deepseek_moe_model.train()\n",
    "    return out\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"\\nStarting DeepSeek-MoE model training...\")\n",
    "train_losses, val_losses = [], []\n",
    "start_time = time.time()\n",
    "\n",
    "for iter_num in range(MAX_ITERS + 1):\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % EVAL_INTERVAL == 0:\n",
    "        # FIX: Call the correctly named estimation function\n",
    "        losses = estimate_loss_deepseek()\n",
    "        val_loss = losses['val']\n",
    "        val_losses.append(val_loss)\n",
    "        train_losses.append(losses['train'])\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {val_loss:.4f}, lr {lr:.6f}\")\n",
    "\n",
    "    X, Y = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    with CTX:\n",
    "        # FIX: Use the correct model variable name here.\n",
    "        logits, loss, router_inputs = deepseek_moe_model(X, Y)\n",
    "\n",
    "    # Backward pass and optimizer step\n",
    "    SCALER.scale(loss).backward()\n",
    "    SCALER.step(optimizer)\n",
    "    SCALER.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # --- Auxiliary-Loss-Free Bias Update ---\n",
    "    with torch.no_grad():\n",
    "        # FIX: And here.\n",
    "        for i, block in enumerate(deepseek_moe_model.transformer.h):\n",
    "            moe_layer = block.moe\n",
    "            x_flat = router_inputs[i].view(-1, MODEL_CONFIG.n_embd)\n",
    "            # Recalculate router scores with current bias for selection\n",
    "            s = moe_layer._affinity(x_flat)\n",
    "            sel_scores = s + moe_layer.router_bias.to(s.device)\n",
    "            topk_indices = torch.topk(sel_scores, moe_layer.top_k, dim=-1).indices\n",
    "            \n",
    "            counts = torch.bincount(topk_indices.view(-1), minlength=moe_layer.n_routed_experts).float()\n",
    "            \n",
    "            avg_load = counts.sum() / moe_layer.n_routed_experts\n",
    "            load_violation = (counts - avg_load) / (avg_load + 1e-6)\n",
    "            moe_layer.router_bias -= moe_layer.bias_update_gamma * load_violation\n",
    "\n",
    "end_time = time.time()\n",
    "deepseek_train_time = (end_time - start_time)/60\n",
    "print(f\"\\nDeepSeek-MoE Training finished in {deepseek_train_time:.2f} minutes.\")\n",
    "\n",
    "# --- Store results for comparison ---\n",
    "# This part is now correct because the variable 'deepseek_moe_model' has been used throughout.\n",
    "comparison_results['deepseek_moe'] = {\n",
    "    'model': deepseek_moe_model.cpu(),\n",
    "    'val_losses': val_losses,\n",
    "    'train_time_minutes': deepseek_train_time\n",
    "}\n",
    "print(\"Phase 3 Complete: DeepSeek-MoE model trained and results stored.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clean up GPU memory\n",
    "import gc\n",
    "del deepseek_moe_model, optimizer, router_inputs, loss, logits, X, Y\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STAGE 6: FINAL COMPREHENSIVE MODEL COMPARISON\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# --- 1. Plot Validation Loss Comparison ---\n",
    "print(\"\\n--- Plotting Validation Loss Comparison ---\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for model_name, results in comparison_results.items():\n",
    "    val_losses = results['val_losses']\n",
    "    steps = [i * EVAL_INTERVAL for i in range(len(val_losses))]\n",
    "    plt.plot(steps, val_losses, marker='o', linestyle='-', label=f'{model_name.upper()} Validation Loss')\n",
    "\n",
    "plt.title('Model Performance Comparison', fontsize=16)\n",
    "plt.xlabel('Training Iterations', fontsize=12)\n",
    "plt.ylabel('Validation Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--')\n",
    "plt.ylim(top=4.0, bottom=min(min(v['val_losses']) for v in comparison_results.values()) - 0.1) # Adjust y-axis for better view\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 2. Calculate and Display Advanced Metrics ---\n",
    "print(\"\\n--- Comprehensive Performance Summary ---\")\n",
    "summary_data = []\n",
    "\n",
    "for model_name, results in comparison_results.items():\n",
    "    val_losses = np.array(results['val_losses'])\n",
    "    \n",
    "    # Best validation loss\n",
    "    best_loss = val_losses.min()\n",
    "    \n",
    "    # Training throughput\n",
    "    train_time_min = results['train_time_minutes']\n",
    "    throughput = MAX_ITERS / train_time_min if train_time_min > 0 else 0\n",
    "    \n",
    "    # Parameter count\n",
    "    num_params_m = results['model'].get_num_params() / 1e6\n",
    "    \n",
    "    summary_data.append({\n",
    "        \"Model\": model_name.upper(),\n",
    "        \"Parameters (M)\": f\"{num_params_m:.2f}\",\n",
    "        \"Training Time (min)\": f\"{train_time_min:.2f}\",\n",
    "        \"Throughput (iter/min)\": f\"{throughput:.1f}\",\n",
    "        \"Best Val Loss\": f\"{best_loss:.4f}\",\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "# --- 3. Final Analysis & Conclusion ---\n",
    "print(\"\\n--- Final Analysis & Conclusion ---\")\n",
    "try:\n",
    "    # Sort models by best validation loss\n",
    "    sorted_by_loss = sorted(summary_data, key=lambda x: float(x['Best Val Loss']))\n",
    "    best_model = sorted_by_loss[0]\n",
    "    standard_moe = [d for d in summary_data if d['Model'] == 'STANDARD_MOE'][0]\n",
    "    deepseek_moe = [d for d in summary_data if d['Model'] == 'DEEPSEEK_MOE'][0]\n",
    "\n",
    "    print(f\"üèÜ Best Performance: {best_model['Model']} achieved the lowest validation loss of {best_model['Best Val Loss']}.\")\n",
    "    print(f\"üöÄ Architectural Efficiency: The DEEPSEEK_MOE model trained faster ({deepseek_moe['Training Time (min)']} min) and had a higher throughput ({deepseek_moe['Throughput (iter/min)']} iter/min) than the STANDARD_MOE model ({standard_moe['Throughput (iter/min)']} iter/min), despite both having a similar number of total parameters.\")\n",
    "    \n",
    "    print(\"\\nüí° Key Takeaway: The DeepSeek-MoE architecture, with its combination of powerful shared experts and smaller specialist experts, proved to be more effective. It not only learned better (lower loss) but was also more computationally efficient during training compared to a standard MoE of a comparable parameter count.\")\n",
    "\n",
    "except (IndexError, ValueError) as e:\n",
    "    print(\"Could not generate automated analysis. Please check the training results.\")\n",
    "\n",
    "\n",
    "print(\"\\nNotebook run complete.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STAGE 7: EXPERT UTILIZATION ANALYSIS (DEEPSEEK-MOE)\n",
    "# =============================================================================\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n--- Visualizing Expert Utilization for DeepSeek-MoE ---\")\n",
    "\n",
    "# Load the trained DeepSeek-MoE model from our results\n",
    "model = comparison_results['deepseek_moe']['model']\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Get a sample batch and run a forward pass in analysis mode\n",
    "with torch.no_grad():\n",
    "    X, Y = get_batch('val')\n",
    "    logits, loss, _ = model(X, Y, analysis_mode=True)\n",
    "\n",
    "# Extract the router indices from the first MoE layer\n",
    "# Shape is (batch_size * block_size, top_k)\n",
    "router_indices = model.transformer.h[0].moe.router_indices.cpu().numpy()\n",
    "\n",
    "# Count how many tokens were sent to each expert\n",
    "expert_counts = np.bincount(router_indices.flatten(), minlength=MODEL_CONFIG.moe_n_routed_experts)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x=np.arange(MODEL_CONFIG.moe_n_routed_experts), y=expert_counts)\n",
    "plt.title('Expert Selection Frequency in a Sample Batch (Layer 0)', fontsize=16)\n",
    "plt.xlabel('Expert ID', fontsize=12)\n",
    "plt.ylabel('Number of Tokens Routed', fontsize=12)\n",
    "plt.xticks(np.arange(MODEL_CONFIG.moe_n_routed_experts))\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThis visualization shows the router's behavior in the first layer for a single validation batch.\")\n",
    "print(\"A good distribution suggests that the model is effectively using multiple specialized experts to process the data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17153d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STAGE 5: EXPERT UTILIZATION ANALYSIS (STANDARD-MOE)\n",
    "# =============================================================================\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n--- Visualizing Expert Utilization for Standard-MoE ---\")\n",
    "\n",
    "# --- Redefine StandardMoE classes with analysis_mode to extract router indices ---\n",
    "# This is necessary because the original class doesn't store this information.\n",
    "class StandardMoELayerWithAnalysis(StandardMoELayer):\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__(config)\n",
    "        self.router_indices = None\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, analysis_mode=False):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.view(-1, C)\n",
    "        num_tokens = x_flat.shape[0]\n",
    "        router_logits = self.router(x_flat)\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        topk_weights, topk_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        \n",
    "        if analysis_mode:\n",
    "            self.router_indices = topk_indices.detach().cpu()\n",
    "            \n",
    "        gates = (topk_weights / topk_weights.sum(dim=-1, keepdim=True)).to(x.dtype)\n",
    "        expert_counts = torch.zeros(self.n_routed_experts, device=x.device)\n",
    "        expert_counts.index_add_(0, topk_indices.view(-1), torch.ones_like(topk_indices.view(-1), dtype=torch.float))\n",
    "        f_i = expert_counts / num_tokens\n",
    "        p_i = routing_weights.mean(dim=0)\n",
    "        aux_loss = self.n_routed_experts * torch.sum(p_i * f_i) * self.aux_loss_coef\n",
    "        final_output_flat = torch.zeros_like(x_flat)\n",
    "        for i in range(self.n_routed_experts):\n",
    "            mask = (topk_indices == i)\n",
    "            row_idx, which_k = mask.nonzero(as_tuple=True)\n",
    "            if row_idx.numel() == 0: continue\n",
    "            expert_in = x_flat.index_select(0, row_idx)\n",
    "            expert_out = self.experts[i](expert_in)\n",
    "            gate_values = gates[row_idx, which_k].unsqueeze(1)\n",
    "            final_output_flat.index_add_(0, row_idx, expert_out * gate_values)\n",
    "        return final_output_flat.view(B, T, C), aux_loss\n",
    "\n",
    "class StandardMoEBlockWithAnalysis(StandardMoEBlock):\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__(config)\n",
    "        self.moe = StandardMoELayerWithAnalysis(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, analysis_mode=False):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        moe_out, aux_loss = self.moe(self.ln_2(x), analysis_mode=analysis_mode)\n",
    "        x = x + moe_out\n",
    "        return x, aux_loss\n",
    "\n",
    "class StandardMoEGPTWithAnalysis(StandardMoEGPT):\n",
    "    def __init__(self, config: DeepSeekMoEConfig):\n",
    "        super().__init__(config)\n",
    "        self.transformer.h = nn.ModuleList([StandardMoEBlockWithAnalysis(config) for _ in range(config.n_layer)])\n",
    "\n",
    "    def forward(self, idx, targets=None, analysis_mode=False):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        total_aux_loss = 0.0\n",
    "        for block in self.transformer.h:\n",
    "            x, aux_loss = block(x, analysis_mode=analysis_mode)\n",
    "            total_aux_loss += aux_loss\n",
    "        x = self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            main_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            total_loss = main_loss + total_aux_loss / self.config.n_layer\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            total_loss = None\n",
    "        return logits, total_loss\n",
    "# --- End of redefinitions ---\n",
    "\n",
    "# Create an instance of the new model class and load the trained weights\n",
    "std_moe_config = DeepSeekMoEConfig()\n",
    "std_moe_config.moe_n_routed_experts = 22 # Ensure it matches the trained model\n",
    "model = StandardMoEGPTWithAnalysis(std_moe_config)\n",
    "model.load_state_dict(comparison_results['standard_moe']['model'].state_dict())\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Get a sample batch and run a forward pass in analysis mode\n",
    "with torch.no_grad():\n",
    "    X, Y = get_batch('val')\n",
    "    logits, loss = model(X, Y, analysis_mode=True)\n",
    "\n",
    "# Extract the router indices from the first MoE layer\n",
    "router_indices = model.transformer.h[0].moe.router_indices.cpu().numpy()\n",
    "\n",
    "# Count how many tokens were sent to each expert\n",
    "expert_counts = np.bincount(router_indices.flatten(), minlength=std_moe_config.moe_n_routed_experts)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x=np.arange(std_moe_config.moe_n_routed_experts), y=expert_counts)\n",
    "plt.title('Standard MoE: Expert Selection Frequency (Layer 0)', fontsize=16)\n",
    "plt.xlabel('Expert ID', fontsize=12)\n",
    "plt.ylabel('Number of Tokens Routed', fontsize=12)\n",
    "plt.xticks(np.arange(std_moe_config.moe_n_routed_experts))\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca732ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
