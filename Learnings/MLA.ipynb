{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64dfa3fd",
   "metadata": {},
   "source": [
    "# Multi-Head Latent Attention (MLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f12c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLA Layer successful!\n",
      "Input shape: torch.Size([4, 64, 512])\n",
      "Output shape: torch.Size([4, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "# Building the MLA Module from scratch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Multi-Head Latent Attention (MLA) as described\n",
    "    in the DeepSeek architecture. This version focuses on the core\n",
    "    \"compress for storage, decompress for use\" mechanism for the\n",
    "    Key and Value matrices.\n",
    "    \"\"\"\n",
    "     \n",
    "    def  __init__(self , d_model , num_heads , d_latent , dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0 , \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.d_latent = d_latent # The dimension of the compressed latent space\n",
    "\n",
    "        # The Query projection remains standard, projecting to the full model dimension.\n",
    "        self.W_q = nn.Linear(d_model , d_model)\n",
    "\n",
    "        # The new KV Down-Projector. This is \"compress\" step.\n",
    "        # It projects the input down to a small , shared latent space.\n",
    "        self.W_dkv = nn.Linear(d_model , d_latent)\n",
    "\n",
    "        # The new Key and Value Up-Projectors. This is the \"decompress\" step.\n",
    "        # They reconstruct the full-sized K and V from latent space.\n",
    "        # Note: These are multi-headed to preserve head diversity.\n",
    "        self.W_uk = nn.Linear(d_latent , d_model)\n",
    "        self.W_uv = nn.Linear(d_latent , d_model)\n",
    "\n",
    "        # The final output projection , standard for multi-head attention.\n",
    "        self.W_o = nn.Linear(d_model , d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Causal mask to prevent attending to future tokens. Using a fixed size for demo.\n",
    "        self.register_buffer('mask' , torch.triu(torch.ones(1,1,1024,1024) , diagonal=1).bool())\n",
    "\n",
    "    \n",
    "    def forward(self , x):\n",
    "        batch_size , seq_len , _ = x.shape\n",
    "\n",
    "        # 1. Query Path (Unchanged)\n",
    "        # Project and reshape the query as in standard MHA.\n",
    "        q = self.W_q(x).view(batch_size , seq_len , self.num_heads , self.d_head).transpose(1,2)\n",
    "\n",
    "        # 2. Key/Value Path (The MLA Innovotion)\n",
    "        # Step 2a: Down-Project to the latent space.\n",
    "        # This is the ONLY value that would be cached during inference.\n",
    "        c_kv = self.W_dkv(x) # Shape: (batch , seq_len , d_latent)\n",
    "\n",
    "        # Step 2b: Up-Project from the latent space to get full K and V.\n",
    "        # These are compute on the fly and are not cached.\n",
    "        k = self.W_uk(c_kv).view(batch_size , seq_len , self.num_heads , self.d_head).transpose(1,2)\n",
    "        v = self.W_uv(c_kv).view(batch_size , seq_len , self.num_heads , self.d_head).transpose(1,2)\n",
    "\n",
    "        # 3. Standard Attention Calculation\n",
    "        # The rest of process is identical to standard MHA.\n",
    "        attn_scores = (q @ k.transpose(-2 , -1)) / (self.d_head ** 0.5)\n",
    "\n",
    "        # Apply causal mask\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask[: , : , :seq_len , :seq_len] , float('-inf')\n",
    "        )\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores , dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = (attn_weights @ v).transpose(1,2).contiguous().view(batch_size , seq_len , self.d_model)\n",
    "\n",
    "        # 4. Final Output Projection\n",
    "        output = self.W_o(context_vector)\n",
    "        return output\n",
    "    \n",
    "# --- Usage Example ---\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_latent = 128  # Latent dimension must be smaller than d_model\n",
    "batch_size = 4\n",
    "seq_len = 64\n",
    "\n",
    "# Instantiate the layer\n",
    "mla_layer = MultiHeadLatentAttention(d_model, num_heads, d_latent)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Pass the input through the layer\n",
    "output = mla_layer(dummy_input)\n",
    "\n",
    "print(\"MLA Layer successful!\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6861ae",
   "metadata": {},
   "source": [
    "# Fused MLA with Decoupled RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689cafe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeekAttention Layer successful!\n",
      "Input shape: torch.Size([4, 64, 512])\n",
      "Output shape: torch.Size([4, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "# Building the Fused MLA and Decoupled RoPE Module\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper module to apply Rotary Positional Encoding (RoPE).\n",
    "    This is not added to the embeddings but applied directly to\n",
    "    the Query and Key vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self , d_head , max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        # Precompute the theta values for rotational matrix\n",
    "        theta = 1.0 / (10000 ** (torch.arange(0 , d_head , 2).float() / d_head))\n",
    "        self.register_buffer('theta' , theta)\n",
    "\n",
    "        # Precompute the frequency terms (m * theta) for all positions\n",
    "        positions = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        freqs = positions * self.theta.unsqueeze(0)\n",
    "\n",
    "        # Create the complex number representation for rotation\n",
    "        # the real part is cos(freqs) and imaginary part is sin(freqs)\n",
    "        self.register_buffer(\"freqs_cis\" , torch.polar(torch.ones_like(freqs) , freqs))\n",
    "\n",
    "    def forward(self , x):\n",
    "        # x.shape: (batch , num_heads , seq_len , d_head)\n",
    "        seq_len = x.shape[2]\n",
    "\n",
    "        # Reshape x to treat pairs of dimensions as complex numbers\n",
    "        x_complex = x.float().view(*x.shape[:-1] , -1 , 2) # shape: (batch , num_heads , seq_len , d_head/2 , 2)\n",
    "        # Convert to PyTorch complex type\n",
    "        x_complex = torch.view_as_complex(x_complex)\n",
    "\n",
    "        # Get the precomputed frequencies for the current sequence length\n",
    "        freqs_cis = self.freqs_cis[:seq_len , :].unsqueeze(0).unsqueeze(0) # shape: (1 , 1 , seq_len , d_head/2)\n",
    "\n",
    "        # Apply rotation by multiplying in the complex domain\n",
    "        # This rotates each pair of dimensions by the angle m * theta_i\n",
    "        x_rotated = x_complex * freqs_cis\n",
    "\n",
    "        # Convert back to real number representation\n",
    "        x_rotated = torch.view_as_real(x_rotated)\n",
    "        # Reshape back to the original d_head dimension\n",
    "        x_rotated = x_rotated.flatten(3)\n",
    "\n",
    "        return x_rotated.type_as(x)\n",
    "    \n",
    "\n",
    "class DeepSeekAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    The full, state-of-the-art attention mechanism from DeepSeek, combining\n",
    "    Multi-Head Latent Attention (MLA) with Decoupled Rotary Positional\n",
    "    Encoding (RoPE).\n",
    "    \"\"\"\n",
    "    def __init__(self , d_model , num_heads , d_latent , d_rope , dropout=0.0 , max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0 , \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.d_latent = d_latent\n",
    "        self.d_rope = d_rope # Dimension for positional vectors\n",
    "\n",
    "        # --- A: Content Path (Pure MLA) ---\n",
    "        self.W_q_content = nn.Linear(d_model , d_model)\n",
    "        self.W_dkv_content = nn.Linear(d_model , d_latent)\n",
    "        self.W_uk_content = nn.Linear(d_latent , d_model)\n",
    "        self.W_uv_content = nn.Linear(d_latent , d_model)\n",
    "\n",
    "        # --- B: Position Path (RoPE Applied) ---\n",
    "        self.W_k_pos = nn.Linear(d_model , d_rope * num_heads)\n",
    "        self.W_q_pos = nn.Linear(d_model , d_rope * num_heads)\n",
    "\n",
    "        # RoPE module to apply the rotations\n",
    "        self.rope = RotaryPositionalEncoding(d_rope , max_seq_len)\n",
    "\n",
    "        # --- C: Final Output Projection ---\n",
    "        self.W_o = nn.Linear(d_model , d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask' , torch.triu(torch.ones(1,1,max_seq_len , max_seq_len) , diagonal=1).bool())\n",
    "\n",
    "    \n",
    "    def forward(self , x):\n",
    "        batch_size , seq_len , _ = x.shape\n",
    "\n",
    "        # --- A: Content Path Calculation ---\n",
    "        # This path is cache-friendly and position-agnostic.\n",
    "        q_c = self.W_q_content(x).view(batch_size , seq_len , self.num_heads , self.d_head).transpose(1,2)\n",
    "        c_kv = self.W_dkv_content(x) # This is what gets cached for content path.\n",
    "        k_c = self.W_uk_content(c_kv).view(batch_size , seq_len , self.num_heads , self.d_head).transpose(1,2)\n",
    "        v_c = self.W_uv_content(c_kv).view(batch_size , seq_len , self.num_heads , self.d_head).transpose(1,2)\n",
    "\n",
    "        # --- B: Position Path Calculation ---\n",
    "        # This path handles the positional information.\n",
    "        q_r_unrotated = self.W_q_pos(x).view(batch_size , seq_len , self.num_heads , self.d_head).transpose(1,2)\n",
    "        k_r_unrotated = self.W_k_pos(x).view(batch_size , seq_len , self.num_heads , self.d_head).transpose(1,2)\n",
    "\n",
    "        # Apply RoPE to the positional Query and Key vectors\n",
    "        q_r = self.rope(q_r_unrotated)\n",
    "        k_r = self.rope(k_r_unrotated) # This is what gets cached for the position path.\n",
    "\n",
    "        # --- C: Combining Paths for Final Attention Score ---\n",
    "        # The final score is the sum of content and position scores.\n",
    "        content_scores = (q_c @ k_c.transpose(-2 , -1)) / (self.d_head ** 0.5)\n",
    "        position_scores = (q_r @ k_r.transpose(-2 , -1)) / (self.d_head ** 0.5)\n",
    "\n",
    "        attn_scores = content_scores + position_scores\n",
    "\n",
    "        # --- D: Final Steps (Masking , SoftMax , Output) ---\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask[: , : , :seq_len , :seq_len] , float('-inf')\n",
    "        )\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores , dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # The final context vector is computed using only the content value matrix (v_c)\n",
    "        context_vector = (attn_weights @ v_c).transpose(1,2).contiguous().view(batch_size , seq_len , self.d_model)\n",
    "\n",
    "        output = self.W_o(context_vector)\n",
    "\n",
    "        return output\n",
    "    \n",
    "# --- Usage Example ---\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_latent = 128\n",
    "d_rope = 64 # Dimension for RoPE, typically d_head or smaller\n",
    "batch_size = 4\n",
    "seq_len = 64\n",
    "\n",
    "# Instantiate the full attention layer\n",
    "deepseek_attn_layer = DeepSeekAttention(d_model, num_heads, d_latent, d_rope)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Pass the input through the layer\n",
    "output = deepseek_attn_layer(dummy_input)\n",
    "\n",
    "print(\"DeepSeekAttention Layer successful!\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2bc85d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
