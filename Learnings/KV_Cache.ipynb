{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e59d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pandhari/Desktop/DeepSeek-from-Scratch/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models...\n",
      "Setup complete.\n",
      "Prompt: 'The next day is bright' and sunny, and the sun is shining. The sun is shining, and the moon is shining.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from transformers import GPT2LMHeadModel , GPT2Tokenizer\n",
    "\n",
    "print(\"Setting up models...\")\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "print(\"Setup complete.\")\n",
    "\n",
    "prompt = \"The next day is bright\"\n",
    "inputs = tokenizer(prompt , return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\" , end=\"\")\n",
    "\n",
    "# Generate 20 tokens\n",
    "for _ in range(20):\n",
    "    # Pass the entire sequence to the model\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get the logits for the very last token\n",
    "    next_token_logits = logits[: , -1 , :]\n",
    "\n",
    "    # Get the ID of the most lokely next token (greedy decoding)\n",
    "    next_token_id = torch.argmax(next_token_logits , dim=-1).unsqueeze(-1)\n",
    "\n",
    "    input_ids = torch.cat([input_ids , next_token_id] , dim=-1)\n",
    "\n",
    "    new_token = tokenizer.decode(next_token_id[0])\n",
    "    print(new_token , end=\"\" , flush=True)\n",
    "    \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7afd9ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating without KV Cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time without KV Cache: 5.3277 seconds\n",
      "\n",
      "Generating with KV Cache...\n",
      "Time with KV Cache: 1.8773 seconds\n",
      "\n",
      "KV Cache Speedup: 2.84x\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The next day is bright\"\n",
    "inputs = tokenizer(prompt , return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "# Timing without KV Cache \n",
    "print(\"Generating without KV Cache...\")\n",
    "start_time_without_cache = time.time()\n",
    "output_without_cache = model.generate(\n",
    "    input_ids , \n",
    "    max_new_tokens=100 , \n",
    "    use_cache=False , \n",
    "    attention_mask=attention_mask\n",
    ")\n",
    "end_time_without_cache = time.time()\n",
    "duration_without_cache = end_time_without_cache - start_time_without_cache\n",
    "print(f\"Time without KV Cache: {duration_without_cache:.4f} seconds\\n\")\n",
    "\n",
    "# --- Timing with KV cache ---\n",
    "print(\"Generating with KV Cache...\")\n",
    "start_time_with_cache = time.time()\n",
    "output_with_cache = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    use_cache=True, # Explicitly enable the cache\n",
    "    attention_mask=attention_mask\n",
    ")\n",
    "end_time_with_cache = time.time()\n",
    "duration_with_cache = end_time_with_cache - start_time_with_cache\n",
    "print(f\"Time with KV Cache: {duration_with_cache:.4f} seconds\\n\")\n",
    "\n",
    "\n",
    "# --- Calculate and print the speedup ---\n",
    "speedup = duration_without_cache / duration_with_cache\n",
    "print(f\"KV Cache Speedup: {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc930d",
   "metadata": {},
   "source": [
    "# MHA W KV_Cache AND W/O KV_Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb16576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216c27b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using Device:\" , device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "291950cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA_NoCache(nn.Module):\n",
    "    def __init__(self , d_model , num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.h = num_heads\n",
    "        self.d = d_model // num_heads\n",
    "\n",
    "        self.q = nn.Linear(d_model , d_model)\n",
    "        self.k = nn.Linear(d_model , d_model)\n",
    "        self.v = nn.Linear(d_model , d_model)\n",
    "        self.o = nn.Linear(d_model , d_model)\n",
    "\n",
    "    def forward(self , x):\n",
    "        B , T , D = x.shape\n",
    "\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "\n",
    "        Q = Q.reshape(B , T , self.h , self.d).transpose(1 , 2)\n",
    "        K = K.reshape(B , T , self.h , self.d).transpose(1 , 2)\n",
    "        V = V.reshape(B , T , self.h , self.d).transpose(1 , 2)\n",
    "\n",
    "        scores = (Q @ K.transpose(-2 , -1)) / math.sqrt(self.d)\n",
    "        attn = torch.softmax(scores , dim=-1)\n",
    "        out = attn @ V\n",
    "\n",
    "        out = out.transpose(1 , 2).contiguous().view(B , T , D)\n",
    "        return self.o(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b03bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b3cca70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH = 1\n",
    "SEQ_LEN = 512       # try 1024 or 2048 if GPU\n",
    "D_MODEL = 512\n",
    "HEADS = 8\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db75abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No KV cache time: 1.1585 seconds\n"
     ]
    }
   ],
   "source": [
    "mha_no_cache = MHA_NoCache(D_MODEL , HEADS).to(device)\n",
    "\n",
    "x = torch.randn(BATCH , 1 , D_MODEL , device=device)\n",
    "tokens = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for t in range(SEQ_LEN):\n",
    "    tokens.append(x)\n",
    "    full_seq = torch.cat(tokens, dim=1)\n",
    "    _ = mha_no_cache(full_seq)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "time_no_cache = time.time() - start\n",
    "print(f\"No KV cache time: {time_no_cache:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee5f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA_KVCache(nn.Module):\n",
    "    def __init__(self , d_model , num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.h = num_heads\n",
    "        self.d = d_model // num_heads\n",
    "\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self , x , kv_cache=None):\n",
    "        B , T , D = x.shape\n",
    "        Q = self.q(x)\n",
    "        K = self.k(x)\n",
    "        V = self.v(x)\n",
    "        \n",
    "        Q = Q.view(B, T, self.h, self.d).transpose(1, 2)\n",
    "        K = K.view(B, T, self.h, self.d).transpose(1, 2)\n",
    "        V = V.view(B, T, self.h, self.d).transpose(1, 2)\n",
    "\n",
    "        if kv_cache is None:\n",
    "            K_cache, V_cache = K, V\n",
    "        else:\n",
    "            K_cache, V_cache = kv_cache\n",
    "            K_cache = torch.cat([K_cache, K], dim=2)\n",
    "            V_cache = torch.cat([V_cache, V], dim=2)\n",
    "\n",
    "        scores = (Q @ K_cache.transpose(-2, -1)) / math.sqrt(self.d)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = attn @ V_cache\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.o(out), (K_cache, V_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee53e307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With KV cache time: 0.1249 seconds\n"
     ]
    }
   ],
   "source": [
    "mha_kv = MHA_KVCache(D_MODEL, HEADS).to(device)\n",
    "\n",
    "x = torch.randn(BATCH, 1, D_MODEL, device=device)\n",
    "kv_cache = None\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for t in range(SEQ_LEN):\n",
    "    _, kv_cache = mha_kv(x, kv_cache)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "time_kv = time.time() - start\n",
    "print(f\"With KV cache time: {time_kv:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa135d",
   "metadata": {},
   "source": [
    "# MQA -> Multi Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e20cb0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MQA Layer successful!\n",
      "Input shape: torch.Size([4, 64, 512])\n",
      "Output shape: torch.Size([4, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self , d_model , num_heads , dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0 , \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model , d_model)\n",
    "        self.W_k = nn.Linear(d_model , self.d_head) # Single projection for K\n",
    "        self.W_v = nn.Linear(d_model , self.d_head) # Single projection for V\n",
    "        self.W_o = nn.Linear(d_model , d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Using a fixed size mask for demonstration. A dynamic one is better in practice.\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(1, 1, 1024, 1024), diagonal=1))\n",
    "    \n",
    "    def forward(self , x):\n",
    "        batch_size , seq_len , _ = x.shape\n",
    "\n",
    "        # Query: (B , num_heads , seq_len , d_head)\n",
    "        q = self.W_q(x).view(batch_size , seq_len , self.num_heads , self.d_head).transpose(1 , 2)\n",
    "\n",
    "        # Key & Value: (B , 1 , seq_len , d_head)\n",
    "        k = self.W_k(x).view(batch_size , seq_len , 1 , self.d_head).transpose(1,2)\n",
    "        v = self.W_v(x).view(batch_size , seq_len , 1 , self.d_head).transpose(1,2)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2 , -1)) / (self.d_head**0.5)\n",
    "\n",
    "        # Apply causal mask\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[: , : , :seq_len , :seq_len] == 1 , float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores , dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = (attn_weights @ v).transpose(1,2).contiguous().view(batch_size , seq_len , self.d_model)\n",
    "\n",
    "        output = self.W_o(context_vector)\n",
    "        return output\n",
    "    \n",
    "# --- Usage Example ---\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 4\n",
    "seq_len = 64\n",
    "\n",
    "mqa_layer = MultiQueryAttention(d_model , num_heads)\n",
    "dummy_input = torch.randn(batch_size , seq_len , d_model)\n",
    "output = mqa_layer(dummy_input)\n",
    "\n",
    "print(\"MQA Layer successful!\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d1b3d",
   "metadata": {},
   "source": [
    "# GQA -> Grouped Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b46f6ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GQA Layer successful!\n",
      "Input shape: torch.Size([4, 64, 512])\n",
      "Output shape: torch.Size([4, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self , d_model , num_heads , num_groups , dropout=0.0 , max_seq_len:int = 1024):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        assert num_heads % num_groups == 0, \"num_heads must be divisible by num_groups\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model , d_model)\n",
    "        self.W_k = nn.Linear(d_model , self.num_groups * self.d_head) # Grouped projection for K\n",
    "        self.W_v = nn.Linear(d_model , self.num_groups * self.d_head) # Grouped projection for V\n",
    "        self.W_o = nn.Linear(d_model , d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._register_mask_buffer(max_seq_len)\n",
    "\n",
    "    def _register_mask_buffer(self, max_seq_len):\n",
    "        if max_seq_len > 0:\n",
    "            mask = torch.triu(torch.ones(1, 1, max_seq_len, max_seq_len, dtype=torch.bool), diagonal=1)\n",
    "            self.register_buffer(\"causal_mask\", mask, persistent=False)\n",
    "        else:\n",
    "            self.causal_mask = None\n",
    "        \n",
    "    def _get_causal_mask(self, seq_len, device):\n",
    "        if self.causal_mask is not None and self.causal_mask.size(-1) >= seq_len:\n",
    "            return self.causal_mask[:, :, :seq_len, :seq_len]\n",
    "        # Dynamically create mask if needed\n",
    "        return torch.triu(torch.ones(1, 1, seq_len, seq_len, dtype=torch.bool, device=device), diagonal=1)\n",
    "    \n",
    "    def forward(self , x):\n",
    "        B , T , _ = x.shape\n",
    "\n",
    "        # Query: (B , num_heads , T , d_head)\n",
    "        q = self.W_q(x).view(B , T , self.num_heads , self.d_head).transpose(1,2)\n",
    "\n",
    "        # Key & Value: (B , num_groups , T , d_head)\n",
    "        k = self.W_k(x).view(B , T , self.num_groups , self.d_head).transpose(1,2)\n",
    "        v = self.W_v(x).view(B , T , self.num_groups , self.d_head).transpose(1,2)\n",
    "\n",
    "        heads_per_group = self.num_heads // self.num_groups\n",
    "\n",
    "        # Repeat K and V to match query heads\n",
    "        k = k.repeat_interleave(heads_per_group , dim=1) # (B , num_heads , T , d_head)\n",
    "        v = v.repeat_interleave(heads_per_group , dim=1) # (B , num_heads , T , d_head)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2 , -1)) * (self.d_head**-0.5)\n",
    "\n",
    "        causal_mask = self._get_causal_mask(T , x.device)\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask , float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores , dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context = (attn_weights @ v).transpose(1,2).contiguous().view(B , T , self.d_model)\n",
    "\n",
    "        return self.W_o(context)\n",
    "    \n",
    "# --- Usage Example ---\n",
    "d_model = 512\n",
    "num_heads = 32\n",
    "num_groups = 4 # e.g., Llama 2 7B uses 4 groups for 32 heads\n",
    "batch_size = 4\n",
    "seq_len = 64\n",
    "\n",
    "gqa_layer = GroupedQueryAttention(d_model, num_heads, num_groups)\n",
    "dummy_input = torch.randn(batch_size, seq_len, d_model)\n",
    "output = gqa_layer(dummy_input)\n",
    "\n",
    "print(\"GQA Layer successful!\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
